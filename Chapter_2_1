2.3 Importing only a subset of data.
PG 31(13)
use the command line parameter "--where" to specify the sql condition that the imported data should meet.

SQOOP will rpopagate the contents of --where parameter, this is used to generate queries that fetch data.
Any special functions,conversions, or even user-defined functions can be used.
Note: this sql fragment will be propagated into generated queries without any sqoop processing.
Becasue of the parallel processing, any expensive function call will put significant burde in db server.
advanced functions could lock certain tables, which may prevent sqoop transfer data in parallel.
Note: run the expensing filtering query on your database prior to import, and save its output to a temp table and run 
sqoop to import the temp table into hadoop without the  "-- where" parameter.

2.4 - Protecting your password:
two options:
do not specify the pwd with  --password parameter.
first option is  "-P" that will instruct the sqoop to read the password from STD input.
alternately pwd is stored in --password-file.

-P method is very secure, and the pwd is not stored anywhere and is loaded on every sqoop execution directly from the user.
The disadvantage is that this method cannot be automated with a SCRIPT.

Second solution is using the --password-file. this will laod the pwd from any specified file on your HDFS cluster.
TO make the file secure, store the file inside your HOME directory, set file's parmissions to 400, so no one else can fetch it.
this method of storing pwd can be easily automated with a script and recommended.
shell and hadoop commands to create and secure your pwd file:
echo "my_pwd"> sqoop.password
hadoop dfs -put sqoop.pawword /user/$USER/sqoop.password
hadoop dfs -chown 400 /user/$USER/sqoop.password
rm sqoop.password
sqoop import --password-file /user/$USER/sqoop.password ...

make sure there are no extra tailing lines and empty lines at the end of the file.
-------------------------------------------
2.5 - Using a file format other than CSV.
If the default CSV files that sqoop uses , does not suit then you prefer binary a primary format over plain text.

Three different file formats are supported in SQOOP.
TEXT is one format. TWO are Binary. Binary formats are - AVRO and Hadoop's SequenceFile.

Options you need to specify are 
--as-sequencefile
--as-avrodatafile

Binary formats have few benefits over text files.
1) binary formats are a natural fit for storing binary values like Image or PDF docs.
These are more suited for storing characters that are used as separators in the text file.
The down side is - to access the binary data, you need to implement extra functionality or load special libraries in ur app.

SequenceFile - is a special Hadoop file format used to store objects and implements Writable interface.
this format is customized for Mapreduce and thus expects that each record will consists of 
two parts... KEY and VALUE. 
but sqoop does not have the concept of Key-value pairs. so it uses an empty object called NULLWRITABLE in place of the VALUE.
For key , sqoop uses the generated class. this generated class is copied into the directory where sqoop is executed.
This generated class need to be integrated with your application if you need to read a sqoop generated SequenceFile.

Apache Avro is GENERIC data serialization system.
--asavrodatafile - parameter instructs sqopp to use its COMPACT and FAST binary encoded format.
Avro is very generic system that can store any arbitrary data structures.

It uses concept called SCHEMA to describe the what data structures are stored within the file.
The schema file is encoded as JSON so that it is decipherable by the human eye.

the schema is generated automatically uisng the metadata info retrieved from the DB and this scehma is retained in each 
generated file.
Application need to use AVRO libraries in order to open and process data stored as AVRO.
In case of sequence file no need to import any special class, since the meta data  is embedded ini the imported files themselves.
------------------------------
sqoop import --connect jdbc:oracle:thin:@//localhost:1521/orcl --username HR --table REGIONS --target-dir reg2 --as-avrodatafile -P
here using the below qyery the metadata is retrived and the schema is written to REGIONS.AVSC
manager.SqlManager: Executing SQL statement: SELECT t.* FROM REGIONS t WHERE 1=0
15/11/22 03:03:56 INFO mapreduce.DataDrivenImportJob: Writing Avro schema file: /tmp/sqoop-oracle/compile/d25f2b53a27df6d3208a02795729424b/REGIONS.avsc


hadoop fs -lsr /user/oracle/reg2/
Warning: $HADOOP_HOME is deprecated.

-rw-r--r--   1 oracle supergroup          0 2015-11-22 03:04 /user/oracle/reg2/_SUCCESS
drwxr-xr-x   - oracle supergroup          0 2015-11-22 03:04 /user/oracle/reg2/_logs
drwxr-xr-x   - oracle supergroup          0 2015-11-22 03:04 /user/oracle/reg2/_logs/history
-rw-r--r--   1 oracle supergroup      17123 2015-11-22 03:04 /user/oracle/reg2/_logs/history/job_201511220256_0002_1448190240209_oracle_REGIONS.jar
-rw-r--r--   1 oracle supergroup      63577 2015-11-22 03:04 /user/oracle/reg2/_logs/history/job_201511220256_0002_conf.xml
-rw-r--r--   1 oracle supergroup        369 2015-11-22 03:04 /user/oracle/reg2/part-m-00000.avro
-rw-r--r--   1 oracle supergroup        371 2015-11-22 03:04 /user/oracle/reg2/part-m-00001.avro
-rw-r--r--   1 oracle supergroup        367 2015-11-22 03:04 /user/oracle/reg2/part-m-00002.avro
-rw-r--r--   1 oracle supergroup        385 2015-11-22 03:04 /user/oracle/reg2/part-m-00003.avro

the output part files names are .avro
 hadoop fs -cat /user/oracle/reg2/part-m-00000.avro
Warning: $HADOOP_HOME is deprecated.

Objavro.schema�{"type":"record","name":"REGIONS","doc":"Sqoop import of REGIONS","fields":[{"name":"REGION_ID","type":["null","string"],"default":null,"columnName":"REGION_ID","sqlType":"2"},{"name":"REGION_NAME","type":["null","string"],"default":null,"columnName":"REGION_NAME","sqlType":"12"}],"tableName":"REGIONS"}�95=��a�u1��1

json fomat..

----
similarly: 
sqoop import --connect jdbc:oracle:thin:@//localhost:1521/orcl --username HR --table REGIONS --target-dir reg1 --as-sequencefile -P
hadoop fs -cat /user/oracle/reg1/part* shows the binary output..

 hadoop fs -cat /user/oracle/reg1/part*
Warning: $HADOOP_HOME is deprecated.

SEQ!org.apache.hadoop.io.LongWritableREGIONSBXOk�����^tm:�	EuropeSEQ!org.apache.hadoop.io.LongWritableREGIONS�� �Si%'�7��_AmericasSEQ!org.apache.hadoop.io.LongWritableREGIONS)un��ԧx�1cAsiaSEQ!org.apache.hadoop.io.LongWritableREGIONSo�Nk\$��
                                                                                                                 ��a�Middle East and Africa[oracle@localhost ~]$ 






